{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version: 2.1.0\n"
     ]
    }
   ],
   "source": [
    "#Note if you are using google colab - please go to Runtime -> Change runtime type  and select GPU as Hardware accelerator. This will make notebook run faster.\n",
    "#github link: https://github.com/sanigam/BERT_Medium\n",
    "\n",
    "#Install following libraries before first run. For subsequent runs, you may comment these\n",
    "# !pip install tensorflow==2.1.0\n",
    "# !pip install tensorflow_hub\n",
    "# !pip install bert-for-tf2\n",
    "# !pip install sentencepiece\n",
    "\n",
    "#Importing Required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import tensorflow_hub as hub\n",
    "from bert import bert_tokenization\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from  sklearn.utils import shuffle\n",
    "#Tensorflow version\n",
    "print(\"Tensorflow Version:\", tf.__version__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21417, 5)\n",
      "(23481, 5)\n",
      "(44898, 5)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('True.csv')\n",
    "df['target'] =1\n",
    "print(df.shape)\n",
    "df1 = pd.read_csv('Fake.csv')\n",
    "df1['target'] =0\n",
    "print(df1.shape)\n",
    "df = df.append (df1)\n",
    "df = shuffle(df)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>target</th>\n",
       "      <th>text1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19331</th>\n",
       "      <td>Civilians leave IS-area in eastern Syria after...</td>\n",
       "      <td>BEIRUT (Reuters) - Hundreds of civilians left ...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>September 22, 2017</td>\n",
       "      <td>1</td>\n",
       "      <td>Civilians leave IS-area in eastern Syria after...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2157</th>\n",
       "      <td>Trump's Strategic and Policy Forum to disband:...</td>\n",
       "      <td>(Reuters) - Members of U.S. President Donald T...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>August 16, 2017</td>\n",
       "      <td>1</td>\n",
       "      <td>Trump's Strategic and Policy Forum to disband:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2720</th>\n",
       "      <td>Virginia Seeks To Hold Trump In Contempt For ...</td>\n",
       "      <td>Donald Trump could end up behind bars if the s...</td>\n",
       "      <td>News</td>\n",
       "      <td>February 2, 2017</td>\n",
       "      <td>0</td>\n",
       "      <td>Virginia Seeks To Hold Trump In Contempt For ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17607</th>\n",
       "      <td>New Zealand kingmaker party to hold key board ...</td>\n",
       "      <td>WELLINGTON (Reuters) - The leader of the small...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>October 12, 2017</td>\n",
       "      <td>1</td>\n",
       "      <td>New Zealand kingmaker party to hold key board ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7990</th>\n",
       "      <td>Washington Post calls for Maine governor to re...</td>\n",
       "      <td>BOSTON (Reuters) - The Washington Post on Wedn...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>September 28, 2016</td>\n",
       "      <td>1</td>\n",
       "      <td>Washington Post calls for Maine governor to re...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "19331  Civilians leave IS-area in eastern Syria after...   \n",
       "2157   Trump's Strategic and Policy Forum to disband:...   \n",
       "2720    Virginia Seeks To Hold Trump In Contempt For ...   \n",
       "17607  New Zealand kingmaker party to hold key board ...   \n",
       "7990   Washington Post calls for Maine governor to re...   \n",
       "\n",
       "                                                    text       subject  \\\n",
       "19331  BEIRUT (Reuters) - Hundreds of civilians left ...     worldnews   \n",
       "2157   (Reuters) - Members of U.S. President Donald T...  politicsNews   \n",
       "2720   Donald Trump could end up behind bars if the s...          News   \n",
       "17607  WELLINGTON (Reuters) - The leader of the small...     worldnews   \n",
       "7990   BOSTON (Reuters) - The Washington Post on Wedn...  politicsNews   \n",
       "\n",
       "                      date  target  \\\n",
       "19331  September 22, 2017        1   \n",
       "2157      August 16, 2017        1   \n",
       "2720      February 2, 2017       0   \n",
       "17607    October 12, 2017        1   \n",
       "7990   September 28, 2016        1   \n",
       "\n",
       "                                                   text1  \n",
       "19331  Civilians leave IS-area in eastern Syria after...  \n",
       "2157   Trump's Strategic and Policy Forum to disband:...  \n",
       "2720    Virginia Seeks To Hold Trump In Contempt For ...  \n",
       "17607  New Zealand kingmaker party to hold key board ...  \n",
       "7990   Washington Post calls for Maine governor to re...  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text1'] = df['title'] + ' . ' + df['text']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Loading newss Data, you can load from file in your computer or from github link. \n",
    "# #For a different use case, you can load different file in similar format\n",
    "\n",
    "# #df = pd.read_csv('newss.csv') #From your computer\n",
    "# df = pd.read_csv('https://raw.githubusercontent.com/sanigam/BERT_Medium/master/newss.csv') #From github link\n",
    "\n",
    "\n",
    "# print ( f'Data Shape: {df.shape} ') #Number of rows and column in data-frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>target</th>\n",
       "      <th>text1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15731</th>\n",
       "      <td>Ex-Argentina VP Boudou arrested in corruption ...</td>\n",
       "      <td>BUENOS AIRES (Reuters) - Amado Boudou, who had...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>November 3, 2017</td>\n",
       "      <td>1</td>\n",
       "      <td>Ex-Argentina VP Boudou arrested in corruption ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17043</th>\n",
       "      <td>Col. Ralph Peters On Obama’s Refusal To Live I...</td>\n",
       "      <td>Peters is dead on in his description of Obama ...</td>\n",
       "      <td>Government News</td>\n",
       "      <td>Oct 13, 2015</td>\n",
       "      <td>0</td>\n",
       "      <td>Col. Ralph Peters On Obama’s Refusal To Live I...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "15731  Ex-Argentina VP Boudou arrested in corruption ...   \n",
       "17043  Col. Ralph Peters On Obama’s Refusal To Live I...   \n",
       "\n",
       "                                                    text          subject  \\\n",
       "15731  BUENOS AIRES (Reuters) - Amado Boudou, who had...        worldnews   \n",
       "17043  Peters is dead on in his description of Obama ...  Government News   \n",
       "\n",
       "                    date  target  \\\n",
       "15731  November 3, 2017        1   \n",
       "17043       Oct 13, 2015       0   \n",
       "\n",
       "                                                   text1  \n",
       "15731  Ex-Argentina VP Boudou arrested in corruption ...  \n",
       "17043  Col. Ralph Peters On Obama’s Refusal To Live I...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample Fake News(target = 0):\n",
      "-------------------------------\n",
      "FORMER DEMOCRAT WARNS Young Americans: “Rioters are being manipulated by big government forces who need them to regain political power” [VIDEO] .  Who is silencing political speech, physically attacking those with divergent views, demanding that every American share one single  ideology. But sadly, the real truth is that these violent agitators have little idea about why they march. Theirs is a movement with no cause, a temper tantrum with no purpose. They are a generation lost in space .a propaganda arm of the party of disappointment. If I were wrong, would Hillary Clinton really be their standard bearer? These rioters are being manipulated by big government forces who need them to regain political power \n",
      "\n",
      "Sample True News (target = 1):\n",
      "--------------------------------------\n",
      "Afghan Shi'ites fear further attacks on Ashura celebrations . KABUL (Reuters) - The Afghan capital Kabul braced on Saturday for further possible attacks ahead of Ashura, the holiest day on the Shi ite Muslim calendar, a day after an attack claimed by Islamic State that killed at least five people near a large Shi ite mosque. Ahead of the celebration on Sunday, signs of increased security were in evidence across Kabul, with extra police checkpoints and roadblocks in many areas, while security was also increased in other cities. Afghanistan, a majority Sunni Muslim country, has traditionally not suffered the sectarian violence that has devastated countries like Iraq, but a series of attacks over recent years have targeted the Shi ite community.  We are concerned about this. We had internal fighting in the past but never religious fighting,  said Arif Rahmani, a member of parliament and a member of the mainly Shi ite Hazara community that has been particularly targeted. The government has provided some basic training and weapons for a few hundred volunteer guards near mosques and other meeting places but many fear that the protection, which covers only some of the city s more than 400 Shi ite mosques, is insufficient. In 2011, more than 80 people were killed in Ashura attacks in Kabul and the northern city of Mazar-i-Sharif and there have been a string of others since, with 20 people killed in a suicide attack on a mosque in Kabul a month ago. Friday s attack, by suicide bombers posing as shepherds walking their sheep along a road outside the Hussainya mosque in the Qala-e-Fatehullah area of the city, did not reach the mosque itself but wounded 20 people in addition to the five killed. No up-to-date census data exists for Afghanistan but different estimates put the size of the Shi ite community at between 10-20 percent of the population, mostly Persian-speaking Tajiks and Hazaras. Ashura, on the 10th day of the month of Muharram, celebrates the martyrdom of Hussein, one of the grandsons of the Prophet Mohammad, and is marked by large public commemorations by Shi ite Muslims. President Ashraf Ghani condemned Friday s attack and said it would not break the unity between religions in Afghanistan. But at a time when rivalry between the patchwork of different ethnic groups in the country has increasingly come into the open, Rahmani said the evident objective of the attacks was to ratchet up the tensions to create instability.  In the past, there were warnings that there were groups that wanted to stir ethnic and religious conflict among Afghans but now it is reality,  Rahmani said.  There are people who want to create disunity among ethnic and religious groups,  he said. \n",
      "\n",
      "True (1)  and Fake News (0)\n",
      "------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Optional Step: Looking into data\n",
    "display(df.sample(2)) #Sample rows of dataframe\n",
    "\n",
    "print ( '\\nSample Fake News(target = 0):\\n-------------------------------')\n",
    "print ( df[df['target']==0].text1.values[100] )\n",
    "\n",
    "print ( '\\nSample True News (target = 1):\\n--------------------------------------')\n",
    "print ( df[df['target']==1].text1.values[100] )\n",
    "\n",
    "print ( '\\nTrue (1)  and Fake News (0)\\n------------------------------------------------------------------------')\n",
    "df['target'].hist() ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Shape: (35918, 6)  ,  Test Data Shape:  (8980, 6)\n"
     ]
    }
   ],
   "source": [
    "# Splitting data into test and training\n",
    "df_train, df_test = train_test_split( df , test_size=0.20, random_state=42)\n",
    "\n",
    "print( f'Training Data Shape: {df_train.shape}  ,  Test Data Shape:  {df_test.shape}') # Rows/Cols in train/test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Length of vocab in our tokenizer :  30522\n"
     ]
    }
   ],
   "source": [
    "#Loading BERT Standard model (Pretrained Model on Wikipedia and Book Corpus)\n",
    "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",trainable=True, name = 'keras_bert_layer' )\n",
    "\n",
    "#Getting vocab file from bert layer\n",
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy() \n",
    "\n",
    "do_lower_case = True  # For uncased model it will True \n",
    "\n",
    "#Defining tokenizer object which will be used to tokenize text before feeding to bert\n",
    "tokenizer_for_bert = bert_tokenization.FullTokenizer(vocab_file, do_lower_case) #Tokenizer to tokenize input text\n",
    "\n",
    "print ( '\\nLength of vocab in our tokenizer : ' , len(tokenizer_for_bert.vocab) ) #BERT vocab has around 30K words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to encode text in format to feed to BERT\n",
    "\n",
    "def encode_text_for_bert (texts, tokenizer_for_bert, max_len=512):\n",
    "    ''' This function is to encode data for inputting into BERT model\n",
    "    Parameters:\n",
    "    texts - List of texts to encode\n",
    "    tokenizer_for_bert - Tokenizer to be used to convert text into tokens\n",
    "    max_len - Maximum length of text. It can have maximum value as 512\n",
    "    Return: Tupple of 3 numpy arrays \n",
    "    1) Token Ids padded with 0s to make length as max_len.  \n",
    "    2) Array where we have 1 for actual tokens and 0 for padding tokens\n",
    "    3) Array of 0s to indicate that token belongs to 1st sentence (chunk of text). There is no 2nd sentence here.\n",
    "    '''\n",
    "    all_token_ids = []\n",
    "    all_masks = []\n",
    "    all_segments = []\n",
    "    \n",
    "    for text in texts:\n",
    "        tokens = tokenizer_for_bert.tokenize(text) #Tokenizing using Bert tokenizer\n",
    "            \n",
    "        tokens = tokens[:max_len-2] # Truncating number of tokens to max_len -2, Reduced extra 2 to add special tokens\n",
    "        \n",
    "        input_sequence = [\"[CLS]\"] + tokens + [\"[SEP]\"]  # [CLS] and [SEP] are special tokens to be added into input text\n",
    "        \n",
    "        pad_len = max_len - len(input_sequence) # Spaces to fill with 0s to make each sequence equal to max_len\n",
    "        \n",
    "        token_ids = tokenizer_for_bert.convert_tokens_to_ids(input_sequence)   #Converting tokens to token ids \n",
    "       \n",
    "        token_ids += [0] * pad_len  #Padding token ids with 0s\n",
    "        \n",
    "        pad_masks = [1] * len(input_sequence) + [0] * pad_len # 1 where we have sentence tokens and 0 otherwise\n",
    "        \n",
    "        segment_ids = [0] * max_len # Segment ids are all 0 to indicate it is part of sentence 1. There is no sentence 2 here\n",
    "        \n",
    "        all_token_ids.append(token_ids)\n",
    "        all_masks.append(pad_masks)\n",
    "        all_segments.append(segment_ids)\n",
    "    \n",
    "    return np.array(all_token_ids), np.array(all_masks), np.array(all_segments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test text after tokenization:  ['[CLS]', 'welcome', 'to', 'bert', 'session', '[SEP]']\n",
      "Test text after encoding:  (array([[  101,  6160,  2000, 14324,  5219,   102,     0]]), array([[1, 1, 1, 1, 1, 1, 0]]), array([[0, 0, 0, 0, 0, 0, 0]]))\n"
     ]
    }
   ],
   "source": [
    "#Optional Step: This is just to understand input/output of function encode_text_for_bert\n",
    "test_text =  \"Welcome to  BERT session \"\n",
    "\n",
    "print (\"Test text after tokenization: \" ,  [\"[CLS]\"] + tokenizer_for_bert.tokenize( test_text)  + [\"[SEP]\"] )\n",
    "\n",
    "print (\"Test text after encoding: \" ,encode_text_for_bert ( [test_text], tokenizer_for_bert, 7 ) ) # Pl Note id 101 is for token [CLS] and 102 for token [SEP]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating BERT  Model \n",
    "def bert_model_creation (bert_layer, max_len=512, model_type = 'Classification', num_classes = 2):\n",
    "    '''This function is to create BERT model for Classification or Regession Task\n",
    "    Parameters:\n",
    "    model_type = 'Classification' for classification task or 'Regression' for regression task. \n",
    "    num_classes = Number of classes in classification task. Value of 2 means binary classification. More than 2 for multiclass classification.\n",
    "                  For regression, num_classes parameter is ignored.\n",
    "    Return: Deep Learning Model\n",
    "    Important: You may add additional dense layers in place holder provided as \"***PLACEHOLDFER FOR ADDITIONAL LAYERS****\"\n",
    "    '''   \n",
    "    #Input to bert layer\n",
    "    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n",
    "    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n",
    "\n",
    "    #Output from bert layer\n",
    "    bert_layer_out = bert_layer([input_word_ids, input_mask, segment_ids]) # Python list of 2 tensors with shape (batch_size, 768) and (batch_size, max_len, 768)\n",
    "    \n",
    "    #Extrating Embedding for CLS token comming out of bert layer. Note CLS is the first token\n",
    "    cls_out = bert_layer_out[1][:,0,:] # Getting hidden-state of 1st tokens from second tensor in bert_layer_out, Tensor shape - (batch size, 768) \n",
    "    \n",
    "    \n",
    "    #***PLACEHOLDFER FOR ADDITIONAL LAYERS****. \n",
    "    #Add more layers here if you want. See example below\n",
    "    #cls_out = Dropout(.25) (cls_out)\n",
    "    #cls_out = Dense(500, activation='relu')(cls_out) \n",
    "    \n",
    "    \n",
    "    \n",
    "    #Defines last layer depending on model type and  number of classes. Activation function is used depending on model_type and num_classes\n",
    "    if model_type == 'Classification' :\n",
    "        if num_classes == 2 :\n",
    "            out = Dense(1, activation='sigmoid')(cls_out)     # ** For Binary classification, use sigmoid activation\n",
    "        else:    \n",
    "            out = Dense(num_classes, activation='softmax')(cls_output) # For Multi Class classification, use softmax activation\n",
    "    else:\n",
    "        out = Dense(1, activation='linear')(cls_out)     # For regression, use linear activation\n",
    "    \n",
    "    #Model creation using inputs and output\n",
    "    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out, name='deeplearning_bert__model')\n",
    "    \n",
    "    \n",
    "    \n",
    "    learning_rate = 2e-6 # modify learning rate,as needed\n",
    "    \n",
    "    #Compiles Model depending on model type and number of classes. Loss function as well as metrics is used accordingly\n",
    "    if model_type == 'Classification' :\n",
    "        if num_classes == 2 :\n",
    "            model.compile(Adam(lr= learning_rate), loss='binary_crossentropy', metrics=['acc']) # ** For Binary classification\n",
    "        else:\n",
    "            model.compile(Adam(lr= learning_rate), loss='sparse_categorical_crossentropy', metrics=['sparse_categorical_accuracy']) # For Multi Class classification \n",
    "    else:\n",
    "        model.compile(Adam(lr= learning_rate), loss='mse', metrics=['mse']) # For Regression\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 250 #Max length of text input to model. It can go up to 512. Keeping it small to run it faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"deeplearning_bert__model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_word_ids (InputLayer)     [(None, 250)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_mask (InputLayer)         [(None, 250)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 250)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_bert_layer (KerasLayer)   [(None, 768), (None, 109482241   input_word_ids[0][0]             \n",
      "                                                                 input_mask[0][0]                 \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_2 (Te [(None, 768)]        0           keras_bert_layer[1][1]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            769         tf_op_layer_strided_slice_2[0][0]\n",
      "==================================================================================================\n",
      "Total params: 109,483,010\n",
      "Trainable params: 109,483,009\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Building Classification Model\n",
    "#modify values of model_type and num_classes as per need\n",
    "model = bert_model_creation(bert_layer, max_len=max_len, model_type = 'Classification', num_classes = 2) #binary classification as num_classes = 2\n",
    "\n",
    "#Model Summary. Pl note, there are ~109 Million parameters as it is BERT standard model\n",
    "model.summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding Training Data for BERT.  If you want  preprocessing/cleaning of input text, it should be done before this step\n",
    "train_input = encode_text_for_bert(df_train['text1'].values, tokenizer_for_bert, max_len= max_len)\n",
    "\n",
    "#Output variable as 0s or 1s for binary classification. It can have more distinct values for multi-class classification or continous values for regression\n",
    "y_train = df_train['target'].values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optional step: Checking accuracy on train news before fine-tuning so that we can see improvement by fine tuning\n",
    "#accuracy_score( y_train, np.round(model.predict(train_input)).flatten() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 35918 samples\n",
      "35918/35918 [==============================] - 571s 16ms/sample - loss: 8.9137e-04 - acc: 0.9998\n"
     ]
    }
   ],
   "source": [
    "#Model Training (Fine-tuning for news classification)\n",
    "epochs = 1       #Modify as neded\n",
    "batch_size = 32  #Modify as needed\n",
    "train_history = model.fit(train_input, y_train ,epochs= epochs,batch_size= batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking accuracy on train news\n",
    "#accuracy_score( y_train, np.round(model.predict(train_input)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9985523385300669"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Encoding test data into BERT Format. If you have  preprocessing/cleaning of input text, it should be done before this step\n",
    "\n",
    "test_input = encode_text_for_bert(df_test['text1'].values, tokenizer_for_bert, max_len= max_len)\n",
    "y_test = df_test['target'].values\n",
    "\n",
    "#Checking accuracy on test news. You may be able to improve it by taking bigger length of text, more epochs or by adding more dense layers into the model\n",
    "accuracy_score( y_test, np.round(model.predict(test_input)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "News Text:  U.S. Army General Gus Perna, chief operating officer of Operation Warp Speed, said Saturday he expects the first COVID-19 vaccine shipments will arrive at 145 sites in states across the country on Monday. \n",
      "News Type:  [['Fake']]    Score: [[0.08849972]]\n"
     ]
    }
   ],
   "source": [
    "#Running model on single text. Validating model for True text\n",
    "\n",
    "news = 'U.S. Army General Gus Perna, chief operating officer of Operation Warp Speed, said Saturday he expects the first COVID-19 vaccine shipments will arrive at 145 sites in states across the country on Monday. '\n",
    "\n",
    "\n",
    "\n",
    "prediction = model.predict (  encode_text_for_bert ( [news], tokenizer_for_bert, max_len=max_len) ) \n",
    "print('News Text: ', news)\n",
    "print ( 'News Type: ', np.where(  prediction >= .5 , \"True\", \"Fake\" ) , '   Score:',  prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "News Text:  Bill Gates’ personal doctor told a medical symposium in Seattle that the Microsoft founder refused to vaccinate his children.\n",
      "News Type:  [['Fake']]    Score: [[0.0019539]]\n"
     ]
    }
   ],
   "source": [
    "#Running model on single text. Validating model for fake text\n",
    "\n",
    "news = \"Bill Gates’ personal doctor told a medical symposium in Seattle that the Microsoft founder refused to vaccinate his children.\"\n",
    "\n",
    "prediction = model.predict (  encode_text_for_bert ( [news], tokenizer_for_bert, max_len=max_len) ) \n",
    "print('News Text: ', news)\n",
    "print ( 'News Type: ', np.where(  prediction >= .5 , \"True\", \"Fake\" ) , '   Score:',  prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Fake_news.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown layer: KerasLayer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-114-a9c8c3782200>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Fake_news.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    144\u001b[0m   if (h5py is not None and (\n\u001b[1;32m    145\u001b[0m       isinstance(filepath, h5py.File) or h5py.is_hdf5(filepath))):\n\u001b[0;32m--> 146\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mhdf5_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model_from_hdf5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36mload_model_from_hdf5\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0mmodel_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     model = model_config_lib.model_from_config(model_config,\n\u001b[0;32m--> 168\u001b[0;31m                                                custom_objects=custom_objects)\n\u001b[0m\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0;31m# set weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/model_config.py\u001b[0m in \u001b[0;36mmodel_from_config\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m     53\u001b[0m                     '`Sequential.from_config(config)`?')\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeserialize\u001b[0m  \u001b[0;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mdeserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/serialization.py\u001b[0m in \u001b[0;36mdeserialize\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m    104\u001b[0m       \u001b[0mmodule_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m       \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m       printable_module_name='layer')\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    301\u001b[0m             custom_objects=dict(\n\u001b[1;32m    302\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_GLOBAL_CUSTOM_OBJECTS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m                 list(custom_objects.items())))\n\u001b[0m\u001b[1;32m    304\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mCustomObjectScope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36mfrom_config\u001b[0;34m(cls, config, custom_objects)\u001b[0m\n\u001b[1;32m    935\u001b[0m     \"\"\"\n\u001b[1;32m    936\u001b[0m     input_tensors, output_tensors, created_layers = reconstruct_from_config(\n\u001b[0;32m--> 937\u001b[0;31m         config, custom_objects)\n\u001b[0m\u001b[1;32m    938\u001b[0m     model = cls(inputs=input_tensors, outputs=output_tensors,\n\u001b[1;32m    939\u001b[0m                 name=config.get('name'))\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36mreconstruct_from_config\u001b[0;34m(config, custom_objects, created_layers)\u001b[0m\n\u001b[1;32m   1891\u001b[0m   \u001b[0;31m# First, we create all layers and enqueue nodes to be processed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1892\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mlayer_data\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'layers'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1893\u001b[0;31m     \u001b[0mprocess_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1894\u001b[0m   \u001b[0;31m# Then we process nodes in order of layer depth.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1895\u001b[0m   \u001b[0;31m# Nodes that cannot yet be processed (if the inbound node\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36mprocess_layer\u001b[0;34m(layer_data)\u001b[0m\n\u001b[1;32m   1873\u001b[0m       \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeserialize\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdeserialize_layer\u001b[0m  \u001b[0;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1874\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1875\u001b[0;31m       \u001b[0mlayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeserialize_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1876\u001b[0m       \u001b[0mcreated_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1877\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/serialization.py\u001b[0m in \u001b[0;36mdeserialize\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m    104\u001b[0m       \u001b[0mmodule_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m       \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m       printable_module_name='layer')\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midentifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m     (cls, cls_config) = class_and_config_for_serialized_keras_object(\n\u001b[0;32m--> 292\u001b[0;31m         config, module_objects, custom_objects, printable_module_name)\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'from_config'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mclass_and_config_for_serialized_keras_object\u001b[0;34m(config, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule_objects\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unknown '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mprintable_module_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m': '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mclass_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m   \u001b[0mcls_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'config'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown layer: KerasLayer"
     ]
    }
   ],
   "source": [
    "model = load_model ('Fake_news.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "News Text:  Ebola Epidemic Becomes Global Health Crisis. The deadliest outbreak of Ebola, a deadly hemorrhagic fever, started in West Africa in the spring of 2014 and began to spread rapidly in late summer. Most of the epidemic was contained in three countries -- Guinea, Sierra Leone and Liberia -- though there have been cases in confirmed in at least five other countries, including two diagnosed in the United States. \n",
      "News Type:  [['Fake']]    Score: [[0.2038865]]\n"
     ]
    }
   ],
   "source": [
    "#Running model on single text. Validating model for True text\n",
    "\n",
    "news = 'Ebola Epidemic Becomes Global Health Crisis. The deadliest outbreak of Ebola, a deadly hemorrhagic fever, started in West Africa in the spring of 2014 and began to spread rapidly in late summer. Most of the epidemic was contained in three countries -- Guinea, Sierra Leone and Liberia -- though there have been cases in confirmed in at least five other countries, including two diagnosed in the United States. '\n",
    "\n",
    "\n",
    "\n",
    "prediction = model.predict (  encode_text_for_bert ( [news], tokenizer_for_bert, max_len=max_len) ) \n",
    "print('News Text: ', news)\n",
    "print ( 'News Type: ', np.where(  prediction >= .5 , \"True\", \"Fake\" ) , '   Score:',  prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did OPRAH Just Leave “Nasty” Hillary Wishing She Wouldn’t Have Endorsed Her? [VIDEO] . Four months ago, Oprah, the woman who made her fortune off preying on women s emotions and counting on their inability to think for themselves, gave Hillary s vagina an endorsement. She gave women absolutely no credible reason to support her, but reminded them, when you walk into the voting booth the choice is clear, one candidate has a vagina and the other one does not: Regardless of your politics, it s a seminal moment for women. What this says is, there is no ceiling, that ceiling just went boom! It says anything is possible when you can be leader of the free world. What Oprah s essentially saying to women who hang on her every word is that it doesn t matter if you or your husband don t have a job. It doesn t matter if you live in a town supported by coal that Hillary promised to destroy. It doesn t matter that Hillary left our nation s top secret classified emails exposed for anyone in the world to see, simply because she believed she was above the law, or that every other person in the history of our government has been punished for less serious breaches of our national security.  Much like she tried to convince her viewers in 2008 that American needed a black President, she s now trying to convince her viewers they need to elect our first woman for President. Am I the only woman who finds Oprah s dumbing down of women to be an insult to my intelligence? Yesterday, Oprah weighed in on the election and gave an endorsement for Hillary that was an insult to the intelligence of every woman still clinging to her failed network. She essentially told them, you may not be smart enough to follow the news or to understand the seriousness of the content of the emails being leaked that prove Hillary was using her position as Secretary of State to funnel money into her Clinton Foundation. You may not even be smart enough to understand the seriousness of the recent videos released by James O Keefe of Project Veritas that show Democrats admitting to committing massive voter fraud for decades and inciting violence at Trump rallies. You may not even be smart enough to know these people featured in the undercover videos admit that Hillary and the DNC are fully aware of, and are funding their illegal activities. Yes, these same people are tied to Hillary who admit on tape they were responsible for the unprecedented violence a horrified America witnessed at Donald Trump s rally in her hometown of Chicago But let me tell you, that nasty woman you see on TV just ignore what you see, because, as Oprah states,  She s not coming over to your house! You don t have to like her,  she said.  You don t have to like her. Watch Oprah below. She apparently couldn t offer her female viewers one concrete reason to vote for Hillary, so instead she focused on reasons to overlook her negatives and vote for her anyhow. Is this really an endorsement that will help Hillary Clinton win with thinking women or will they just use her endorsement to point out to other women that Oprah couldn t even think of one good reason to vote for Hillary other than the obvious argument used for the low information voter she has a vagina.Here s a video of Hillary speaking down to a young reporter in Oprah s beloved Africa. Maybe Oprah s decided that it s okay for women in positions of power to talk to women like this, as long as you don t invite them over for dinner:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.00016482]], dtype=float32)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=600\n",
    "print(df_test['text1'].values[i])\n",
    "model.predict (  encode_text_for_bert ( [df_test['text1'].values[i]], tokenizer_for_bert, max_len=max_len) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title      Did OPRAH Just Leave “Nasty” Hillary Wishing S...\n",
       "text       Four months ago, Oprah, the woman who made her...\n",
       "subject                                             politics\n",
       "date                                            Oct 22, 2016\n",
       "target                                                     0\n",
       "text1      Did OPRAH Just Leave “Nasty” Hillary Wishing S...\n",
       "Name: 12664, dtype: object"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.iloc[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "News Text:  Trump is president of USA\n",
      "News Type:  [['Fake']]    Score: [[0.01417429]]\n"
     ]
    }
   ],
   "source": [
    "#Running model on single text. Validating model for True text\n",
    "\n",
    "news = 'Trump is president of USA'\n",
    "\n",
    "\n",
    "prediction = model.predict (  encode_text_for_bert ( [news], tokenizer_for_bert, max_len=max_len) ) \n",
    "print('News Text: ', news)\n",
    "print ( 'News Type: ', np.where(  prediction >= .5 , \"True\", \"Fake\" ) , '   Score:',  prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
